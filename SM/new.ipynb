{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8068b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp39-cp39-win_amd64.whl (212.4 MB)\n",
      "     ------------------------------------ 212.4/212.4 MB 980.0 kB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.0 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.10.0\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.8/45.8 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: typing-extensions, sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.10.1\n",
      "    Uninstalling sympy-1.10.1:\n",
      "      Successfully uninstalled sympy-1.10.1\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 typing-extensions-4.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8382c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C:\\\\Users\\\\JAI SHREE KRISHNA\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mZipImportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_hooks\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;32m<frozen zipimport>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mZipImportError\u001b[0m: not a Zip file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5520\\651102506.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# 1. CONFIGURATION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import seaborn objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\palettes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhusl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdesaturate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmplcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m from pandas.io.api import (\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;31m# excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgbq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_gbq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_orc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_parquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.io.json._json import (\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdumps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mloads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mread_json\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mto_json\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_hooks\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "DATA_DIR      = r'D:\\STUDY\\Sumit\\lectures sumit\\college\\sem 6\\dyslexia\\Dataset Dyslexia_Password WanAsy321\\Gambo\\Train'   # your Train folder\n",
    "BATCH_SIZE    = 64\n",
    "IMG_SIZE      = 64\n",
    "NUM_CLASSES   = 3\n",
    "EPOCHS        = 30\n",
    "LR            = 1e-3\n",
    "PATIENCE      = 5     # for early stopping\n",
    "DEVICE        = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLASS_NAMES   = ['normal','reversal','corrected']\n",
    "\n",
    "# 2. DATA AUGMENTATION & LOADER\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.1), shear=5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),                    # [0,1]\n",
    "    transforms.Normalize([0.5], [0.5]),       # [-1,1]\n",
    "])\n",
    "# We split train into train/val\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# 3. MODEL DEFINITION (Light CNN)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 32×32×32\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 64×16×16\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                # 128×8×8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128*8*8, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# 4. TRAINING SETUP\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "# Early stopping tracker\n",
    "best_val_loss = np.inf\n",
    "patience_ctr  = 0\n",
    "\n",
    "# 5. TRAIN LOOP\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_loss, train_acc, n = 0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # metrics\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_acc += (preds == labels).sum().item()\n",
    "        n += imgs.size(0)\n",
    "    train_loss /= n\n",
    "    train_acc  = train_acc / n\n",
    "\n",
    "    # --- Validate ---\n",
    "    model.eval()\n",
    "    val_loss, val_acc, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_acc += (preds == labels).sum().item()\n",
    "            n += imgs.size(0)\n",
    "    val_loss /= n\n",
    "    val_acc  = val_acc / n\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  \"\n",
    "          f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f}  \"\n",
    "          f\"Val   loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_ctr  = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# 6. EVALUATION ON VALIDATION SET\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0907408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 151649 images belonging to 3 classes.\n",
      "Found 56723 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAI SHREE KRISHNA\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\JAI SHREE KRISHNA\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1139s\u001b[0m 240ms/step - accuracy: 0.8073 - loss: 0.4738 - val_accuracy: 0.8453 - val_loss: 0.4462\n",
      "Epoch 2/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 90ms/step - accuracy: 0.9265 - loss: 0.1997 - val_accuracy: 0.8597 - val_loss: 0.4086\n",
      "Epoch 3/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 97ms/step - accuracy: 0.9453 - loss: 0.1501 - val_accuracy: 0.8232 - val_loss: 0.5522\n",
      "Epoch 4/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 90ms/step - accuracy: 0.9523 - loss: 0.1302 - val_accuracy: 0.8805 - val_loss: 0.3579\n",
      "Epoch 5/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 95ms/step - accuracy: 0.9571 - loss: 0.1150 - val_accuracy: 0.8767 - val_loss: 0.3999\n",
      "Epoch 6/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 119ms/step - accuracy: 0.9618 - loss: 0.1053 - val_accuracy: 0.8956 - val_loss: 0.3271\n",
      "Epoch 7/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 118ms/step - accuracy: 0.9640 - loss: 0.0989 - val_accuracy: 0.8881 - val_loss: 0.3594\n",
      "Epoch 8/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 122ms/step - accuracy: 0.9675 - loss: 0.0919 - val_accuracy: 0.8958 - val_loss: 0.3321\n",
      "Epoch 9/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 127ms/step - accuracy: 0.9682 - loss: 0.0884 - val_accuracy: 0.8751 - val_loss: 0.4039\n",
      "Epoch 10/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 120ms/step - accuracy: 0.9699 - loss: 0.0830 - val_accuracy: 0.8875 - val_loss: 0.3679\n",
      "Epoch 11/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 119ms/step - accuracy: 0.9712 - loss: 0.0802 - val_accuracy: 0.8778 - val_loss: 0.4147\n",
      "Epoch 12/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 116ms/step - accuracy: 0.9722 - loss: 0.0793 - val_accuracy: 0.9091 - val_loss: 0.2877\n",
      "Epoch 13/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38387s\u001b[0m 8s/step - accuracy: 0.9728 - loss: 0.0766 - val_accuracy: 0.8936 - val_loss: 0.3406\n",
      "Epoch 14/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 103ms/step - accuracy: 0.9736 - loss: 0.0738 - val_accuracy: 0.9001 - val_loss: 0.3361\n",
      "Epoch 15/15\n",
      "\u001b[1m4740/4740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 102ms/step - accuracy: 0.9745 - loss: 0.0733 - val_accuracy: 0.9013 - val_loss: 0.3640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "train_dir = r\"D:\\STUDY\\Sumit\\lectures sumit\\college\\sem 6\\dyslexia\\Dataset Dyslexia_Password WanAsy321\\Gambo\\Train\"\n",
    "test_dir = r\"D:\\STUDY\\Sumit\\lectures sumit\\college\\sem 6\\dyslexia\\Dataset Dyslexia_Password WanAsy321\\Gambo\\Test\"\n",
    "\n",
    "# Image params\n",
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators with augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,1)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"reversal_detector.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0593a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow<=2.10\n",
      "  Downloading tensorflow-2.10.0-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "     ------------------------------------ 455.9/455.9 MB 415.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (2.1.1)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (4.13.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.50.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (3.19.6)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.21.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (21.3)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (63.4.1)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (22.10.26)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (0.27.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorflow<=2.10) (3.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<=2.10) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from packaging->tensorflow<=2.10) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.2.2)\n",
      "Installing collected packages: keras, tensorflow-estimator, keras-preprocessing, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.11.0\n",
      "    Can't uninstall 'keras'. No files were found to uninstall.\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.11.0\n",
      "    Can't uninstall 'tensorflow-estimator'. No files were found to uninstall.\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.11.0\n",
      "    Can't uninstall 'tensorboard'. No files were found to uninstall.\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.11.0\n",
      "    Uninstalling tensorflow-2.11.0:\n",
      "      Successfully uninstalled tensorflow-2.11.0\n",
      "  Rolling back uninstall of tensorflow\n",
      "  Moving to c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages\\tensorflow-2.11.0.dist-info\\\n",
      "   from C:\\Users\\JAI SHREE KRISHNA\\anaconda3\\Lib\\site-packages\\~ensorflow-2.11.0.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: 'c:\\\\users\\\\jai shree krishna\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow_estimator-2.11.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for tensorboard: [Errno 2] No such file or directory: 'c:\\\\users\\\\jai shree krishna\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorboard-2.11.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for keras: [Errno 2] No such file or directory: 'c:\\\\users\\\\jai shree krishna\\\\anaconda3\\\\lib\\\\site-packages\\\\keras-2.11.0.dist-info\\\\METADATA'\n",
      "    WARNING: No metadata found in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\jai shree krishna\\anaconda3\\lib\\site-packages\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\JAI SHREE KRISHNA\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\compiler\\\\tf2tensorrt\\\\_pywrap_py_utils.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow \n",
    "!pip install tensorflow==2.10 --user\n",
    "\n",
    "!conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503f7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c800ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3db00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42be150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
